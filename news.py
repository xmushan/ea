import requests
import os
import json
from datetime import datetime,timedelta
from bs4 import BeautifulSoup
from ai import ai

# ====== è‡ªå®šä¹‰å…³é”®è¯åˆ—è¡¨ï¼ˆä½ å¯ä»¥æ ¹æ®äº¤æ˜“å“ç§åŠ¨æ€æŒ‡å®šï¼‰ ======
KEYWORDS = [
    "é»„é‡‘", "ç¾å…ƒ", "ç¾è”å‚¨", "æˆ˜äº‰", "CPI", "ç‰¹æœ—æ™®"
]

now = datetime.now()
cutoff_date = now - timedelta(days=3)  # æœ€è¿‘3å¤©å†…çš„æ–°é—»
API_KEY = "7c9970e14ec3d77c1a32566c7f8cadab"

def extract_main_content(soup):
    candidates = [
        'div.article', 'div.content', 'div#article', 'div#artibody', 'div#main-content', 'article'
    ]
    for selector in candidates:
        container = soup.select_one(selector)
        if container:
            paragraphs = container.find_all('p')
            content = '\n'.join(
                p.get_text().strip() for p in paragraphs
                if p.get_text().strip() and len(p.get_text().strip()) > 10
            )
            if len(content) > 50:
                return content

    all_paragraphs = soup.find_all('p')
    filtered = []
    for p in all_paragraphs:
        text = p.get_text().strip()
        if not text:
            continue
        if any(word in text for word in ['æ¨èé˜…è¯»', 'æ‰«ç ', 'å¾®ä¿¡', 'å¹¿å‘Š', 'åˆ†äº«åˆ°']):
            continue
        filtered.append(text)

    return '\n'.join(filtered)

def query_news_by_keyword(keyword):
    print(f"\nğŸ” æ­£åœ¨æŠ“å–å…³é”®è¯ï¼š{keyword}")
    response = requests.get(
        'https://apis.tianapi.com/world/index',
        params={
            'key': API_KEY,
            'word': keyword
        }
    )

    result_list = []

    if response.status_code == 200:
        data = response.json()
        for item in data.get('result', {}).get('newslist', []):
            # è§£ææ—¶é—´
            ctime_str = item.get('ctime')
            try:
                pub_time = datetime.strptime(ctime_str, "%Y-%m-%d %H:%M")
            except:
                continue  # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™æ¡æ–°é—»

            if pub_time < cutoff_date:
                continue  # å¦‚æœæ—¶é—´æ—©äº3å¤©å‰ï¼Œè·³è¿‡

            url = item.get('url')
            title = item.get('title', '')
            if url:
                try:
                    detail_res = requests.get(url, timeout=10)
                    try:
                        html_text = detail_res.text.encode('latin1').decode('utf-8', errors='ignore')
                    except:
                        html_text = detail_res.text
                    soup = BeautifulSoup(html_text, 'html.parser')
                    main_content = extract_main_content(soup)
                    item['detail_content'] = main_content if main_content else "æœªèƒ½æå–æ­£æ–‡"
                except Exception as e:
                    item['detail_content'] = f"æŠ“å–å¤±è´¥: {e}"
            else:
                item['detail_content'] = "æ— æ­£æ–‡é“¾æ¥"
            
            print(f"ğŸ“° æ ‡é¢˜ï¼š{title}")
            print(f"ğŸ“… æ—¥æœŸï¼š{ctime_str}")
            print(f"ğŸ“„ æ‘˜è¦ï¼š{item['detail_content'][:100]}...\n")

            result_list.append(item)
    else:
        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
    
    return result_list

def get_all_news_and_save():
    all_results = []
    for kw in KEYWORDS:
        news_list = query_news_by_keyword(kw)
        all_results.extend(news_list)

    # ä¿å­˜
    now = datetime.now()
    filename = now.strftime("news_%Y-%m-%d_%H-%M-%S.json")
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… å…¨éƒ¨æ–°é—»å·²ä¿å­˜ï¼š{filename}")
    ai(filename)
